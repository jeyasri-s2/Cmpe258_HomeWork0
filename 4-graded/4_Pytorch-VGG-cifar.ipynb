{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_Pytorch-VGG-cifar.ipynb","provenance":[],"authorship_tag":"ABX9TyMcqKZun0NeyRYdzfiY/BcG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WaCAWljQNOjl","colab_type":"text"},"source":[" # VGG11 Definition  in Pytorch."]},{"cell_type":"code","metadata":{"id":"Huqba1PGdSF4","colab_type":"code","colab":{}},"source":["''''''\n","import torch\n","import torch.nn as nn\n","\n","\n","cfg = {\n","    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n","    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n","    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n","}\n","\n","\n","class VGG(nn.Module):\n","    def __init__(self, vgg_name):\n","        super(VGG, self).__init__()\n","        self.features = self._make_layers(cfg[vgg_name])\n","        self.classifier = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        return out\n","\n","    def _make_layers(self, cfg):\n","        layers = []\n","        in_channels = 3\n","        for x in cfg:\n","            if x == 'M':\n","                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n","            else:\n","                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n","                           nn.BatchNorm2d(x),\n","                           nn.ReLU(inplace=True)]\n","                in_channels = x\n","        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n","        return nn.Sequential(*layers)\n","\n","\n","def test():\n","    net = VGG('VGG11')\n","    x = torch.randn(2,3,32,32)\n","    y = net(x)\n","    print(y.size())\n","\n","# test()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"17MS9byxNJrV","colab_type":"text"},"source":["# Train CIFAR10 with PyTorch.\n"]},{"cell_type":"code","metadata":{"id":"C_sdowsrWRQK","colab_type":"code","outputId":"2c38b7e7-60cd-4e4b-935e-b70630bf6e8f","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","\n","\n","# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n","# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n","# parser.add_argument('--resume', '-r', action='store_true',\n","#                     help='resume from checkpoint')\n","# args = parser.parse_args()\n","\n","lr = 0.01\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# Model\n","print('==> Building model..')\n","net = VGG('VGG16')\n","# net = ResNet18()\n","# net = PreActResNet18()\n","# net = GoogLeNet()\n","# net = DenseNet121()\n","# net = ResNeXt29_2x64d()\n","# net = MobileNet()\n","# net = MobileNetV2()\n","# net = DPN92()\n","# net = ShuffleNetG2()\n","# net = SENet18()\n","# net = ShuffleNetV2(1)\n","# net = EfficientNetB0()\n","# net = RegNetX_200MF()\n","net = net.to(device)\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","# if args.resume:\n","#     # Load checkpoint.\n","#     print('==> Resuming from checkpoint..')\n","#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n","#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n","#     net.load_state_dict(checkpoint['net'])\n","#     best_acc = checkpoint['acc']\n","#     start_epoch = checkpoint['epoch']\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=lr,\n","                      momentum=0.9, weight_decay=5e-4)\n","\n","# Training\n","\n","\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'):\n","            os.mkdir('checkpoint')\n","        torch.save(state, './checkpoint/ckpt.pth')\n","        best_acc = acc\n","\n","\n","for epoch in range(start_epoch, start_epoch+50):\n","    train(epoch)\n","    test(epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","\n","Epoch: 0\n","0 391 Loss: 2.574 | Acc: 8.594% (11/128)\n","1 391 Loss: 2.448 | Acc: 9.766% (25/256)\n","2 391 Loss: 2.409 | Acc: 13.542% (52/384)\n","3 391 Loss: 2.380 | Acc: 14.258% (73/512)\n","4 391 Loss: 2.368 | Acc: 14.688% (94/640)\n","5 391 Loss: 2.343 | Acc: 15.755% (121/768)\n","6 391 Loss: 2.310 | Acc: 16.518% (148/896)\n","7 391 Loss: 2.276 | Acc: 17.090% (175/1024)\n","8 391 Loss: 2.249 | Acc: 17.969% (207/1152)\n","9 391 Loss: 2.217 | Acc: 18.906% (242/1280)\n","10 391 Loss: 2.207 | Acc: 18.750% (264/1408)\n","11 391 Loss: 2.191 | Acc: 19.141% (294/1536)\n","12 391 Loss: 2.171 | Acc: 19.952% (332/1664)\n","13 391 Loss: 2.168 | Acc: 19.866% (356/1792)\n","14 391 Loss: 2.168 | Acc: 19.948% (383/1920)\n","15 391 Loss: 2.161 | Acc: 20.215% (414/2048)\n","16 391 Loss: 2.160 | Acc: 20.496% (446/2176)\n","17 391 Loss: 2.159 | Acc: 20.616% (475/2304)\n","18 391 Loss: 2.144 | Acc: 21.135% (514/2432)\n","19 391 Loss: 2.152 | Acc: 21.094% (540/2560)\n","20 391 Loss: 2.146 | Acc: 20.871% (561/2688)\n","21 391 Loss: 2.136 | Acc: 21.200% (597/2816)\n","22 391 Loss: 2.134 | Acc: 21.501% (633/2944)\n","23 391 Loss: 2.133 | Acc: 21.842% (671/3072)\n","24 391 Loss: 2.119 | Acc: 22.094% (707/3200)\n","25 391 Loss: 2.120 | Acc: 22.206% (739/3328)\n","26 391 Loss: 2.121 | Acc: 22.512% (778/3456)\n","27 391 Loss: 2.116 | Acc: 22.712% (814/3584)\n","28 391 Loss: 2.115 | Acc: 22.683% (842/3712)\n","29 391 Loss: 2.105 | Acc: 22.995% (883/3840)\n","30 391 Loss: 2.107 | Acc: 23.059% (915/3968)\n","31 391 Loss: 2.105 | Acc: 23.413% (959/4096)\n","32 391 Loss: 2.103 | Acc: 23.438% (990/4224)\n","33 391 Loss: 2.105 | Acc: 23.621% (1028/4352)\n","34 391 Loss: 2.113 | Acc: 23.504% (1053/4480)\n","35 391 Loss: 2.110 | Acc: 23.655% (1090/4608)\n","36 391 Loss: 2.118 | Acc: 23.564% (1116/4736)\n","37 391 Loss: 2.122 | Acc: 23.581% (1147/4864)\n","38 391 Loss: 2.123 | Acc: 23.478% (1172/4992)\n","39 391 Loss: 2.119 | Acc: 23.457% (1201/5120)\n","40 391 Loss: 2.116 | Acc: 23.666% (1242/5248)\n","41 391 Loss: 2.109 | Acc: 24.051% (1293/5376)\n","42 391 Loss: 2.111 | Acc: 24.219% (1333/5504)\n","43 391 Loss: 2.111 | Acc: 24.379% (1373/5632)\n","44 391 Loss: 2.108 | Acc: 24.410% (1406/5760)\n","45 391 Loss: 2.107 | Acc: 24.372% (1435/5888)\n","46 391 Loss: 2.109 | Acc: 24.418% (1469/6016)\n","47 391 Loss: 2.112 | Acc: 24.430% (1501/6144)\n","48 391 Loss: 2.111 | Acc: 24.474% (1535/6272)\n","49 391 Loss: 2.112 | Acc: 24.500% (1568/6400)\n","50 391 Loss: 2.109 | Acc: 24.632% (1608/6528)\n","51 391 Loss: 2.109 | Acc: 24.654% (1641/6656)\n","52 391 Loss: 2.109 | Acc: 24.587% (1668/6784)\n","53 391 Loss: 2.109 | Acc: 24.682% (1706/6912)\n","54 391 Loss: 2.106 | Acc: 24.730% (1741/7040)\n","55 391 Loss: 2.108 | Acc: 24.749% (1774/7168)\n","56 391 Loss: 2.112 | Acc: 24.849% (1813/7296)\n","57 391 Loss: 2.118 | Acc: 24.811% (1842/7424)\n","58 391 Loss: 2.116 | Acc: 24.762% (1870/7552)\n","59 391 Loss: 2.116 | Acc: 24.740% (1900/7680)\n","60 391 Loss: 2.113 | Acc: 24.744% (1932/7808)\n","61 391 Loss: 2.110 | Acc: 24.836% (1971/7936)\n","62 391 Loss: 2.107 | Acc: 24.950% (2012/8064)\n","63 391 Loss: 2.105 | Acc: 25.000% (2048/8192)\n","64 391 Loss: 2.101 | Acc: 25.144% (2092/8320)\n","65 391 Loss: 2.097 | Acc: 25.237% (2132/8448)\n","66 391 Loss: 2.096 | Acc: 25.292% (2169/8576)\n","67 391 Loss: 2.091 | Acc: 25.471% (2217/8704)\n","68 391 Loss: 2.086 | Acc: 25.679% (2268/8832)\n","69 391 Loss: 2.082 | Acc: 25.815% (2313/8960)\n","70 391 Loss: 2.079 | Acc: 25.891% (2353/9088)\n","71 391 Loss: 2.075 | Acc: 25.987% (2395/9216)\n","72 391 Loss: 2.068 | Acc: 26.113% (2440/9344)\n","73 391 Loss: 2.063 | Acc: 26.193% (2481/9472)\n","74 391 Loss: 2.057 | Acc: 26.365% (2531/9600)\n","75 391 Loss: 2.052 | Acc: 26.573% (2585/9728)\n","76 391 Loss: 2.045 | Acc: 26.765% (2638/9856)\n","77 391 Loss: 2.041 | Acc: 26.863% (2682/9984)\n","78 391 Loss: 2.039 | Acc: 26.889% (2719/10112)\n","79 391 Loss: 2.035 | Acc: 27.041% (2769/10240)\n","80 391 Loss: 2.031 | Acc: 27.112% (2811/10368)\n","81 391 Loss: 2.027 | Acc: 27.210% (2856/10496)\n","82 391 Loss: 2.022 | Acc: 27.372% (2908/10624)\n","83 391 Loss: 2.019 | Acc: 27.437% (2950/10752)\n","84 391 Loss: 2.016 | Acc: 27.583% (3001/10880)\n","85 391 Loss: 2.011 | Acc: 27.680% (3047/11008)\n","86 391 Loss: 2.006 | Acc: 27.838% (3100/11136)\n","87 391 Loss: 2.004 | Acc: 27.930% (3146/11264)\n","88 391 Loss: 2.002 | Acc: 28.011% (3191/11392)\n","89 391 Loss: 2.001 | Acc: 28.082% (3235/11520)\n","90 391 Loss: 1.998 | Acc: 28.142% (3278/11648)\n","91 391 Loss: 1.994 | Acc: 28.227% (3324/11776)\n","92 391 Loss: 1.994 | Acc: 28.192% (3356/11904)\n","93 391 Loss: 1.990 | Acc: 28.341% (3410/12032)\n","94 391 Loss: 1.990 | Acc: 28.405% (3454/12160)\n","95 391 Loss: 1.988 | Acc: 28.491% (3501/12288)\n","96 391 Loss: 1.987 | Acc: 28.504% (3539/12416)\n","97 391 Loss: 1.984 | Acc: 28.540% (3580/12544)\n","98 391 Loss: 1.981 | Acc: 28.630% (3628/12672)\n","99 391 Loss: 1.981 | Acc: 28.703% (3674/12800)\n","100 391 Loss: 1.978 | Acc: 28.759% (3718/12928)\n","101 391 Loss: 1.974 | Acc: 28.914% (3775/13056)\n","102 391 Loss: 1.971 | Acc: 28.975% (3820/13184)\n","103 391 Loss: 1.969 | Acc: 29.064% (3869/13312)\n","104 391 Loss: 1.968 | Acc: 29.107% (3912/13440)\n","105 391 Loss: 1.965 | Acc: 29.201% (3962/13568)\n","106 391 Loss: 1.961 | Acc: 29.271% (4009/13696)\n","107 391 Loss: 1.958 | Acc: 29.355% (4058/13824)\n","108 391 Loss: 1.956 | Acc: 29.415% (4104/13952)\n","109 391 Loss: 1.955 | Acc: 29.467% (4149/14080)\n","110 391 Loss: 1.953 | Acc: 29.582% (4203/14208)\n","111 391 Loss: 1.949 | Acc: 29.722% (4261/14336)\n","112 391 Loss: 1.947 | Acc: 29.819% (4313/14464)\n","113 391 Loss: 1.945 | Acc: 29.921% (4366/14592)\n","114 391 Loss: 1.943 | Acc: 29.959% (4410/14720)\n","115 391 Loss: 1.939 | Acc: 30.058% (4463/14848)\n","116 391 Loss: 1.936 | Acc: 30.148% (4515/14976)\n","117 391 Loss: 1.933 | Acc: 30.217% (4564/15104)\n","118 391 Loss: 1.931 | Acc: 30.252% (4608/15232)\n","119 391 Loss: 1.928 | Acc: 30.319% (4657/15360)\n","120 391 Loss: 1.927 | Acc: 30.333% (4698/15488)\n","121 391 Loss: 1.924 | Acc: 30.398% (4747/15616)\n","122 391 Loss: 1.920 | Acc: 30.526% (4806/15744)\n","123 391 Loss: 1.917 | Acc: 30.588% (4855/15872)\n","124 391 Loss: 1.915 | Acc: 30.694% (4911/16000)\n","125 391 Loss: 1.912 | Acc: 30.841% (4974/16128)\n","126 391 Loss: 1.910 | Acc: 30.949% (5031/16256)\n","127 391 Loss: 1.908 | Acc: 30.981% (5076/16384)\n","128 391 Loss: 1.905 | Acc: 31.068% (5130/16512)\n","129 391 Loss: 1.903 | Acc: 31.166% (5186/16640)\n","130 391 Loss: 1.901 | Acc: 31.256% (5241/16768)\n","131 391 Loss: 1.899 | Acc: 31.286% (5286/16896)\n","132 391 Loss: 1.898 | Acc: 31.326% (5333/17024)\n","133 391 Loss: 1.895 | Acc: 31.407% (5387/17152)\n","134 391 Loss: 1.892 | Acc: 31.481% (5440/17280)\n","135 391 Loss: 1.891 | Acc: 31.514% (5486/17408)\n","136 391 Loss: 1.889 | Acc: 31.564% (5535/17536)\n","137 391 Loss: 1.887 | Acc: 31.612% (5584/17664)\n","138 391 Loss: 1.885 | Acc: 31.683% (5637/17792)\n","139 391 Loss: 1.884 | Acc: 31.724% (5685/17920)\n","140 391 Loss: 1.882 | Acc: 31.710% (5723/18048)\n","141 391 Loss: 1.880 | Acc: 31.773% (5775/18176)\n","142 391 Loss: 1.878 | Acc: 31.867% (5833/18304)\n","143 391 Loss: 1.875 | Acc: 31.999% (5898/18432)\n","144 391 Loss: 1.873 | Acc: 32.091% (5956/18560)\n","145 391 Loss: 1.872 | Acc: 32.138% (6006/18688)\n","146 391 Loss: 1.870 | Acc: 32.196% (6058/18816)\n","147 391 Loss: 1.869 | Acc: 32.200% (6100/18944)\n","148 391 Loss: 1.866 | Acc: 32.293% (6159/19072)\n","149 391 Loss: 1.864 | Acc: 32.375% (6216/19200)\n","150 391 Loss: 1.864 | Acc: 32.424% (6267/19328)\n","151 391 Loss: 1.863 | Acc: 32.484% (6320/19456)\n","152 391 Loss: 1.860 | Acc: 32.583% (6381/19584)\n","153 391 Loss: 1.857 | Acc: 32.681% (6442/19712)\n","154 391 Loss: 1.856 | Acc: 32.717% (6491/19840)\n","155 391 Loss: 1.854 | Acc: 32.767% (6543/19968)\n","156 391 Loss: 1.850 | Acc: 32.877% (6607/20096)\n","157 391 Loss: 1.847 | Acc: 32.971% (6668/20224)\n","158 391 Loss: 1.845 | Acc: 33.034% (6723/20352)\n","159 391 Loss: 1.843 | Acc: 33.096% (6778/20480)\n","160 391 Loss: 1.841 | Acc: 33.147% (6831/20608)\n","161 391 Loss: 1.840 | Acc: 33.193% (6883/20736)\n","162 391 Loss: 1.838 | Acc: 33.321% (6952/20864)\n","163 391 Loss: 1.834 | Acc: 33.451% (7022/20992)\n","164 391 Loss: 1.832 | Acc: 33.532% (7082/21120)\n","165 391 Loss: 1.829 | Acc: 33.631% (7146/21248)\n","166 391 Loss: 1.827 | Acc: 33.669% (7197/21376)\n","167 391 Loss: 1.825 | Acc: 33.752% (7258/21504)\n","168 391 Loss: 1.823 | Acc: 33.862% (7325/21632)\n","169 391 Loss: 1.821 | Acc: 33.920% (7381/21760)\n","170 391 Loss: 1.819 | Acc: 34.000% (7442/21888)\n","171 391 Loss: 1.818 | Acc: 34.025% (7491/22016)\n","172 391 Loss: 1.815 | Acc: 34.127% (7557/22144)\n","173 391 Loss: 1.813 | Acc: 34.195% (7616/22272)\n","174 391 Loss: 1.811 | Acc: 34.268% (7676/22400)\n","175 391 Loss: 1.808 | Acc: 34.339% (7736/22528)\n","176 391 Loss: 1.806 | Acc: 34.424% (7799/22656)\n","177 391 Loss: 1.804 | Acc: 34.524% (7866/22784)\n","178 391 Loss: 1.803 | Acc: 34.558% (7918/22912)\n","179 391 Loss: 1.800 | Acc: 34.635% (7980/23040)\n","180 391 Loss: 1.799 | Acc: 34.716% (8043/23168)\n","181 391 Loss: 1.796 | Acc: 34.809% (8109/23296)\n","182 391 Loss: 1.794 | Acc: 34.866% (8167/23424)\n","183 391 Loss: 1.793 | Acc: 34.918% (8224/23552)\n","184 391 Loss: 1.791 | Acc: 34.992% (8286/23680)\n","185 391 Loss: 1.788 | Acc: 35.114% (8360/23808)\n","186 391 Loss: 1.786 | Acc: 35.190% (8423/23936)\n","187 391 Loss: 1.784 | Acc: 35.285% (8491/24064)\n","188 391 Loss: 1.782 | Acc: 35.309% (8542/24192)\n","189 391 Loss: 1.780 | Acc: 35.407% (8611/24320)\n","190 391 Loss: 1.779 | Acc: 35.443% (8665/24448)\n","191 391 Loss: 1.778 | Acc: 35.465% (8716/24576)\n","192 391 Loss: 1.775 | Acc: 35.545% (8781/24704)\n","193 391 Loss: 1.773 | Acc: 35.619% (8845/24832)\n","194 391 Loss: 1.771 | Acc: 35.685% (8907/24960)\n","195 391 Loss: 1.769 | Acc: 35.758% (8971/25088)\n","196 391 Loss: 1.767 | Acc: 35.795% (9026/25216)\n","197 391 Loss: 1.766 | Acc: 35.839% (9083/25344)\n","198 391 Loss: 1.764 | Acc: 35.886% (9141/25472)\n","199 391 Loss: 1.762 | Acc: 35.949% (9203/25600)\n","200 391 Loss: 1.760 | Acc: 36.007% (9264/25728)\n","201 391 Loss: 1.758 | Acc: 36.092% (9332/25856)\n","202 391 Loss: 1.756 | Acc: 36.145% (9392/25984)\n","203 391 Loss: 1.755 | Acc: 36.240% (9463/26112)\n","204 391 Loss: 1.752 | Acc: 36.338% (9535/26240)\n","205 391 Loss: 1.751 | Acc: 36.336% (9581/26368)\n","206 391 Loss: 1.749 | Acc: 36.439% (9655/26496)\n","207 391 Loss: 1.748 | Acc: 36.490% (9715/26624)\n","208 391 Loss: 1.746 | Acc: 36.513% (9768/26752)\n","209 391 Loss: 1.745 | Acc: 36.570% (9830/26880)\n","210 391 Loss: 1.744 | Acc: 36.630% (9893/27008)\n","211 391 Loss: 1.742 | Acc: 36.700% (9959/27136)\n","212 391 Loss: 1.740 | Acc: 36.770% (10025/27264)\n","213 391 Loss: 1.739 | Acc: 36.799% (10080/27392)\n","214 391 Loss: 1.737 | Acc: 36.853% (10142/27520)\n","215 391 Loss: 1.736 | Acc: 36.929% (10210/27648)\n","216 391 Loss: 1.735 | Acc: 36.982% (10272/27776)\n","217 391 Loss: 1.733 | Acc: 37.063% (10342/27904)\n","218 391 Loss: 1.731 | Acc: 37.111% (10403/28032)\n","219 391 Loss: 1.730 | Acc: 37.116% (10452/28160)\n","220 391 Loss: 1.729 | Acc: 37.129% (10503/28288)\n","221 391 Loss: 1.727 | Acc: 37.187% (10567/28416)\n","222 391 Loss: 1.726 | Acc: 37.230% (10627/28544)\n","223 391 Loss: 1.724 | Acc: 37.291% (10692/28672)\n","224 391 Loss: 1.722 | Acc: 37.326% (10750/28800)\n","225 391 Loss: 1.721 | Acc: 37.369% (10810/28928)\n","226 391 Loss: 1.720 | Acc: 37.424% (10874/29056)\n","227 391 Loss: 1.718 | Acc: 37.455% (10931/29184)\n","228 391 Loss: 1.718 | Acc: 37.445% (10976/29312)\n","229 391 Loss: 1.716 | Acc: 37.497% (11039/29440)\n","230 391 Loss: 1.715 | Acc: 37.571% (11109/29568)\n","231 391 Loss: 1.713 | Acc: 37.621% (11172/29696)\n","232 391 Loss: 1.712 | Acc: 37.681% (11238/29824)\n","233 391 Loss: 1.710 | Acc: 37.700% (11292/29952)\n","234 391 Loss: 1.710 | Acc: 37.716% (11345/30080)\n","235 391 Loss: 1.708 | Acc: 37.798% (11418/30208)\n","236 391 Loss: 1.707 | Acc: 37.836% (11478/30336)\n","237 391 Loss: 1.706 | Acc: 37.858% (11533/30464)\n","238 391 Loss: 1.705 | Acc: 37.896% (11593/30592)\n","239 391 Loss: 1.703 | Acc: 37.962% (11662/30720)\n","240 391 Loss: 1.702 | Acc: 38.032% (11732/30848)\n","241 391 Loss: 1.700 | Acc: 38.075% (11794/30976)\n","242 391 Loss: 1.698 | Acc: 38.143% (11864/31104)\n","243 391 Loss: 1.697 | Acc: 38.211% (11934/31232)\n","244 391 Loss: 1.696 | Acc: 38.230% (11989/31360)\n","245 391 Loss: 1.694 | Acc: 38.310% (12063/31488)\n","246 391 Loss: 1.693 | Acc: 38.373% (12132/31616)\n","247 391 Loss: 1.691 | Acc: 38.436% (12201/31744)\n","248 391 Loss: 1.689 | Acc: 38.491% (12268/31872)\n","249 391 Loss: 1.687 | Acc: 38.572% (12343/32000)\n","250 391 Loss: 1.686 | Acc: 38.630% (12411/32128)\n","251 391 Loss: 1.685 | Acc: 38.675% (12475/32256)\n","252 391 Loss: 1.684 | Acc: 38.735% (12544/32384)\n","253 391 Loss: 1.682 | Acc: 38.804% (12616/32512)\n","254 391 Loss: 1.681 | Acc: 38.857% (12683/32640)\n","255 391 Loss: 1.679 | Acc: 38.925% (12755/32768)\n","256 391 Loss: 1.678 | Acc: 38.965% (12818/32896)\n","257 391 Loss: 1.677 | Acc: 39.002% (12880/33024)\n","258 391 Loss: 1.676 | Acc: 39.056% (12948/33152)\n","259 391 Loss: 1.675 | Acc: 39.102% (13013/33280)\n","260 391 Loss: 1.673 | Acc: 39.152% (13080/33408)\n","261 391 Loss: 1.671 | Acc: 39.203% (13147/33536)\n","262 391 Loss: 1.670 | Acc: 39.262% (13217/33664)\n","263 391 Loss: 1.669 | Acc: 39.305% (13282/33792)\n","264 391 Loss: 1.669 | Acc: 39.325% (13339/33920)\n","265 391 Loss: 1.667 | Acc: 39.371% (13405/34048)\n","266 391 Loss: 1.666 | Acc: 39.402% (13466/34176)\n","267 391 Loss: 1.665 | Acc: 39.444% (13531/34304)\n","268 391 Loss: 1.663 | Acc: 39.495% (13599/34432)\n","269 391 Loss: 1.662 | Acc: 39.540% (13665/34560)\n","270 391 Loss: 1.660 | Acc: 39.567% (13725/34688)\n","271 391 Loss: 1.659 | Acc: 39.608% (13790/34816)\n","272 391 Loss: 1.658 | Acc: 39.646% (13854/34944)\n","273 391 Loss: 1.657 | Acc: 39.690% (13920/35072)\n","274 391 Loss: 1.655 | Acc: 39.764% (13997/35200)\n","275 391 Loss: 1.654 | Acc: 39.796% (14059/35328)\n","276 391 Loss: 1.653 | Acc: 39.818% (14118/35456)\n","277 391 Loss: 1.652 | Acc: 39.847% (14179/35584)\n","278 391 Loss: 1.651 | Acc: 39.891% (14246/35712)\n","279 391 Loss: 1.650 | Acc: 39.947% (14317/35840)\n","280 391 Loss: 1.648 | Acc: 39.980% (14380/35968)\n","281 391 Loss: 1.648 | Acc: 40.018% (14445/36096)\n","282 391 Loss: 1.647 | Acc: 40.029% (14500/36224)\n","283 391 Loss: 1.645 | Acc: 40.094% (14575/36352)\n","284 391 Loss: 1.644 | Acc: 40.115% (14634/36480)\n","285 391 Loss: 1.643 | Acc: 40.169% (14705/36608)\n","286 391 Loss: 1.643 | Acc: 40.203% (14769/36736)\n","287 391 Loss: 1.641 | Acc: 40.256% (14840/36864)\n","288 391 Loss: 1.640 | Acc: 40.292% (14905/36992)\n","289 391 Loss: 1.639 | Acc: 40.318% (14966/37120)\n","290 391 Loss: 1.638 | Acc: 40.362% (15034/37248)\n","291 391 Loss: 1.636 | Acc: 40.422% (15108/37376)\n","292 391 Loss: 1.635 | Acc: 40.454% (15172/37504)\n","293 391 Loss: 1.634 | Acc: 40.484% (15235/37632)\n","294 391 Loss: 1.633 | Acc: 40.514% (15298/37760)\n","295 391 Loss: 1.632 | Acc: 40.543% (15361/37888)\n","296 391 Loss: 1.631 | Acc: 40.591% (15431/38016)\n","297 391 Loss: 1.629 | Acc: 40.643% (15503/38144)\n","298 391 Loss: 1.628 | Acc: 40.698% (15576/38272)\n","299 391 Loss: 1.626 | Acc: 40.740% (15644/38400)\n","300 391 Loss: 1.625 | Acc: 40.804% (15721/38528)\n","301 391 Loss: 1.624 | Acc: 40.842% (15788/38656)\n","302 391 Loss: 1.623 | Acc: 40.888% (15858/38784)\n","303 391 Loss: 1.622 | Acc: 40.923% (15924/38912)\n","304 391 Loss: 1.621 | Acc: 40.976% (15997/39040)\n","305 391 Loss: 1.620 | Acc: 41.021% (16067/39168)\n","306 391 Loss: 1.618 | Acc: 41.063% (16136/39296)\n","307 391 Loss: 1.617 | Acc: 41.115% (16209/39424)\n","308 391 Loss: 1.615 | Acc: 41.181% (16288/39552)\n","309 391 Loss: 1.614 | Acc: 41.247% (16367/39680)\n","310 391 Loss: 1.613 | Acc: 41.323% (16450/39808)\n","311 391 Loss: 1.611 | Acc: 41.381% (16526/39936)\n","312 391 Loss: 1.610 | Acc: 41.434% (16600/40064)\n","313 391 Loss: 1.609 | Acc: 41.468% (16667/40192)\n","314 391 Loss: 1.608 | Acc: 41.498% (16732/40320)\n","315 391 Loss: 1.607 | Acc: 41.540% (16802/40448)\n","316 391 Loss: 1.606 | Acc: 41.571% (16868/40576)\n","317 391 Loss: 1.605 | Acc: 41.591% (16929/40704)\n","318 391 Loss: 1.604 | Acc: 41.636% (17001/40832)\n","319 391 Loss: 1.603 | Acc: 41.667% (17067/40960)\n","320 391 Loss: 1.602 | Acc: 41.689% (17129/41088)\n","321 391 Loss: 1.601 | Acc: 41.739% (17203/41216)\n","322 391 Loss: 1.600 | Acc: 41.788% (17277/41344)\n","323 391 Loss: 1.599 | Acc: 41.828% (17347/41472)\n","324 391 Loss: 1.597 | Acc: 41.882% (17423/41600)\n","325 391 Loss: 1.596 | Acc: 41.933% (17498/41728)\n","326 391 Loss: 1.595 | Acc: 41.987% (17574/41856)\n","327 391 Loss: 1.595 | Acc: 42.021% (17642/41984)\n","328 391 Loss: 1.594 | Acc: 42.033% (17701/42112)\n","329 391 Loss: 1.593 | Acc: 42.069% (17770/42240)\n","330 391 Loss: 1.592 | Acc: 42.105% (17839/42368)\n","331 391 Loss: 1.591 | Acc: 42.129% (17903/42496)\n","332 391 Loss: 1.590 | Acc: 42.159% (17970/42624)\n","333 391 Loss: 1.590 | Acc: 42.173% (18030/42752)\n","334 391 Loss: 1.589 | Acc: 42.204% (18097/42880)\n","335 391 Loss: 1.588 | Acc: 42.264% (18177/43008)\n","336 391 Loss: 1.587 | Acc: 42.313% (18252/43136)\n","337 391 Loss: 1.586 | Acc: 42.342% (18319/43264)\n","338 391 Loss: 1.584 | Acc: 42.386% (18392/43392)\n","339 391 Loss: 1.583 | Acc: 42.456% (18477/43520)\n","340 391 Loss: 1.582 | Acc: 42.492% (18547/43648)\n","341 391 Loss: 1.581 | Acc: 42.539% (18622/43776)\n","342 391 Loss: 1.579 | Acc: 42.604% (18705/43904)\n","343 391 Loss: 1.578 | Acc: 42.653% (18781/44032)\n","344 391 Loss: 1.577 | Acc: 42.699% (18856/44160)\n","345 391 Loss: 1.575 | Acc: 42.784% (18948/44288)\n","346 391 Loss: 1.574 | Acc: 42.825% (19021/44416)\n","347 391 Loss: 1.573 | Acc: 42.839% (19082/44544)\n","348 391 Loss: 1.572 | Acc: 42.886% (19158/44672)\n","349 391 Loss: 1.571 | Acc: 42.940% (19237/44800)\n","350 391 Loss: 1.569 | Acc: 42.982% (19311/44928)\n","351 391 Loss: 1.568 | Acc: 43.022% (19384/45056)\n","352 391 Loss: 1.567 | Acc: 43.057% (19455/45184)\n","353 391 Loss: 1.567 | Acc: 43.070% (19516/45312)\n","354 391 Loss: 1.565 | Acc: 43.110% (19589/45440)\n","355 391 Loss: 1.565 | Acc: 43.109% (19644/45568)\n","356 391 Loss: 1.564 | Acc: 43.146% (19716/45696)\n","357 391 Loss: 1.563 | Acc: 43.176% (19785/45824)\n","358 391 Loss: 1.563 | Acc: 43.219% (19860/45952)\n","359 391 Loss: 1.561 | Acc: 43.255% (19932/46080)\n","360 391 Loss: 1.560 | Acc: 43.283% (20000/46208)\n","361 391 Loss: 1.560 | Acc: 43.297% (20062/46336)\n","362 391 Loss: 1.559 | Acc: 43.330% (20133/46464)\n","363 391 Loss: 1.558 | Acc: 43.355% (20200/46592)\n","364 391 Loss: 1.557 | Acc: 43.416% (20284/46720)\n","365 391 Loss: 1.555 | Acc: 43.466% (20363/46848)\n","366 391 Loss: 1.554 | Acc: 43.509% (20439/46976)\n","367 391 Loss: 1.554 | Acc: 43.546% (20512/47104)\n","368 391 Loss: 1.553 | Acc: 43.572% (20580/47232)\n","369 391 Loss: 1.552 | Acc: 43.585% (20642/47360)\n","370 391 Loss: 1.551 | Acc: 43.645% (20726/47488)\n","371 391 Loss: 1.550 | Acc: 43.700% (20808/47616)\n","372 391 Loss: 1.548 | Acc: 43.746% (20886/47744)\n","373 391 Loss: 1.547 | Acc: 43.769% (20953/47872)\n","374 391 Loss: 1.547 | Acc: 43.810% (21029/48000)\n","375 391 Loss: 1.545 | Acc: 43.846% (21102/48128)\n","376 391 Loss: 1.544 | Acc: 43.889% (21179/48256)\n","377 391 Loss: 1.543 | Acc: 43.926% (21253/48384)\n","378 391 Loss: 1.542 | Acc: 43.969% (21330/48512)\n","379 391 Loss: 1.541 | Acc: 44.021% (21412/48640)\n","380 391 Loss: 1.540 | Acc: 44.066% (21490/48768)\n","381 391 Loss: 1.538 | Acc: 44.118% (21572/48896)\n","382 391 Loss: 1.537 | Acc: 44.146% (21642/49024)\n","383 391 Loss: 1.536 | Acc: 44.202% (21726/49152)\n","384 391 Loss: 1.534 | Acc: 44.255% (21809/49280)\n","385 391 Loss: 1.533 | Acc: 44.303% (21889/49408)\n","386 391 Loss: 1.533 | Acc: 44.315% (21952/49536)\n","387 391 Loss: 1.531 | Acc: 44.362% (22032/49664)\n","388 391 Loss: 1.531 | Acc: 44.405% (22110/49792)\n","389 391 Loss: 1.529 | Acc: 44.453% (22191/49920)\n","390 391 Loss: 1.528 | Acc: 44.480% (22240/50000)\n","0 100 Loss: 1.460 | Acc: 50.000% (50/100)\n","1 100 Loss: 1.587 | Acc: 48.500% (97/200)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zYybxC_re_VR","colab_type":"text"},"source":["Accuracy of the model is **48%**\n","\n","CIFAR10 dataset is trained with PyTorch"]}]}
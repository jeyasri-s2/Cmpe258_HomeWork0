{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_Pytorch_Resnet.ipynb","provenance":[],"authorship_tag":"ABX9TyMKM6wyLhkq7EWOfgvjDL01"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uNmaUwyvMvYL","colab_type":"text"},"source":["# Resnet implementation with PyTorch"]},{"cell_type":"code","metadata":{"id":"W0lr78nDaPb6","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2,2,2,2])\n","\n","def ResNet34():\n","    return ResNet(BasicBlock, [3,4,6,3])\n","\n","def ResNet50():\n","    return ResNet(Bottleneck, [3,4,6,3])\n","\n","def ResNet101():\n","    return ResNet(Bottleneck, [3,4,23,3])\n","\n","def ResNet152():\n","    return ResNet(Bottleneck, [3,8,36,3])\n","\n","\n","def test():\n","    net = ResNet18()\n","    y = net(torch.randn(1,3,32,32))\n","    print(y.size())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S13GiXuhM1TS","colab_type":"text"},"source":["# Resnet18 Definition with PyTorch"]},{"cell_type":"code","metadata":{"id":"X_a3x_dAaUU6","colab_type":"code","outputId":"3d32d9a0-a07a-42af-f26c-6da96e9030c9","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import argparse\n","\n","\n","# parser = argparse.ArgumentParser(description='PyTorch CIFAR10 Training')\n","# parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n","# parser.add_argument('--resume', '-r', action='store_true',\n","#                     help='resume from checkpoint')\n","# args = parser.parse_args()\n","\n","lr = 0.01\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","best_acc = 0  # best test accuracy\n","start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n","\n","# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(\n","    root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(\n","    root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","# Model\n","print('==> Building model..')\n","# net = VGG('VGG16')\n","net = ResNet18()\n","# net = PreActResNet18()\n","# net = GoogLeNet()\n","# net = DenseNet121()\n","# net = ResNeXt29_2x64d()\n","# net = MobileNet()\n","# net = MobileNetV2()\n","# net = DPN92()\n","# net = ShuffleNetG2()\n","# net = SENet18()\n","# net = ShuffleNetV2(1)\n","# net = EfficientNetB0()\n","# net = RegNetX_200MF()\n","net = net.to(device)\n","if device == 'cuda':\n","    net = torch.nn.DataParallel(net)\n","    cudnn.benchmark = True\n","\n","# if args.resume:\n","#     # Load checkpoint.\n","#     print('==> Resuming from checkpoint..')\n","#     assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n","#     checkpoint = torch.load('./checkpoint/ckpt.pth')\n","#     net.load_state_dict(checkpoint['net'])\n","#     best_acc = checkpoint['acc']\n","#     start_epoch = checkpoint['epoch']\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=lr,\n","                      momentum=0.9, weight_decay=5e-4)\n","\n","# Training\n","\n","\n","def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        outputs = net(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets).sum().item()\n","\n","        print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","\n","def test(epoch):\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(testloader):\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': net.state_dict(),\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir('checkpoint'):\n","            os.mkdir('checkpoint')\n","        torch.save(state, './checkpoint/ckpt.pth')\n","        best_acc = acc\n","\n","\n","for epoch in range(start_epoch, start_epoch+50):\n","    train(epoch)\n","    test(epoch)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n","==> Building model..\n","\n","Epoch: 0\n","0 391 Loss: 2.383 | Acc: 13.281% (17/128)\n","1 391 Loss: 2.339 | Acc: 12.109% (31/256)\n","2 391 Loss: 2.314 | Acc: 14.844% (57/384)\n","3 391 Loss: 2.285 | Acc: 14.453% (74/512)\n","4 391 Loss: 2.300 | Acc: 13.906% (89/640)\n","5 391 Loss: 2.273 | Acc: 14.583% (112/768)\n","6 391 Loss: 2.247 | Acc: 15.290% (137/896)\n","7 391 Loss: 2.240 | Acc: 15.820% (162/1024)\n","8 391 Loss: 2.226 | Acc: 16.753% (193/1152)\n","9 391 Loss: 2.207 | Acc: 17.422% (223/1280)\n","10 391 Loss: 2.187 | Acc: 18.395% (259/1408)\n","11 391 Loss: 2.177 | Acc: 18.750% (288/1536)\n","12 391 Loss: 2.160 | Acc: 19.772% (329/1664)\n","13 391 Loss: 2.145 | Acc: 20.089% (360/1792)\n","14 391 Loss: 2.141 | Acc: 20.625% (396/1920)\n","15 391 Loss: 2.124 | Acc: 21.387% (438/2048)\n","16 391 Loss: 2.107 | Acc: 21.967% (478/2176)\n","17 391 Loss: 2.112 | Acc: 22.005% (507/2304)\n","18 391 Loss: 2.114 | Acc: 22.410% (545/2432)\n","19 391 Loss: 2.101 | Acc: 22.734% (582/2560)\n","20 391 Loss: 2.094 | Acc: 22.768% (612/2688)\n","21 391 Loss: 2.086 | Acc: 22.940% (646/2816)\n","22 391 Loss: 2.079 | Acc: 22.996% (677/2944)\n","23 391 Loss: 2.080 | Acc: 22.982% (706/3072)\n","24 391 Loss: 2.072 | Acc: 23.312% (746/3200)\n","25 391 Loss: 2.066 | Acc: 23.407% (779/3328)\n","26 391 Loss: 2.053 | Acc: 24.016% (830/3456)\n","27 391 Loss: 2.043 | Acc: 24.414% (875/3584)\n","28 391 Loss: 2.045 | Acc: 24.461% (908/3712)\n","29 391 Loss: 2.041 | Acc: 24.740% (950/3840)\n","30 391 Loss: 2.042 | Acc: 24.773% (983/3968)\n","31 391 Loss: 2.039 | Acc: 24.976% (1023/4096)\n","32 391 Loss: 2.033 | Acc: 25.237% (1066/4224)\n","33 391 Loss: 2.034 | Acc: 25.345% (1103/4352)\n","34 391 Loss: 2.029 | Acc: 25.558% (1145/4480)\n","35 391 Loss: 2.026 | Acc: 25.608% (1180/4608)\n","36 391 Loss: 2.023 | Acc: 25.739% (1219/4736)\n","37 391 Loss: 2.019 | Acc: 25.925% (1261/4864)\n","38 391 Loss: 2.017 | Acc: 25.962% (1296/4992)\n","39 391 Loss: 2.018 | Acc: 25.918% (1327/5120)\n","40 391 Loss: 2.012 | Acc: 26.086% (1369/5248)\n","41 391 Loss: 2.005 | Acc: 26.395% (1419/5376)\n","42 391 Loss: 2.001 | Acc: 26.453% (1456/5504)\n","43 391 Loss: 1.997 | Acc: 26.545% (1495/5632)\n","44 391 Loss: 1.992 | Acc: 26.597% (1532/5760)\n","45 391 Loss: 1.990 | Acc: 26.630% (1568/5888)\n","46 391 Loss: 1.988 | Acc: 26.729% (1608/6016)\n","47 391 Loss: 1.981 | Acc: 27.035% (1661/6144)\n","48 391 Loss: 1.977 | Acc: 27.248% (1709/6272)\n","49 391 Loss: 1.972 | Acc: 27.359% (1751/6400)\n","50 391 Loss: 1.969 | Acc: 27.390% (1788/6528)\n","51 391 Loss: 1.964 | Acc: 27.479% (1829/6656)\n","52 391 Loss: 1.961 | Acc: 27.639% (1875/6784)\n","53 391 Loss: 1.957 | Acc: 27.734% (1917/6912)\n","54 391 Loss: 1.952 | Acc: 27.855% (1961/7040)\n","55 391 Loss: 1.948 | Acc: 28.069% (2012/7168)\n","56 391 Loss: 1.944 | Acc: 28.235% (2060/7296)\n","57 391 Loss: 1.941 | Acc: 28.314% (2102/7424)\n","58 391 Loss: 1.938 | Acc: 28.297% (2137/7552)\n","59 391 Loss: 1.934 | Acc: 28.451% (2185/7680)\n","60 391 Loss: 1.929 | Acc: 28.612% (2234/7808)\n","61 391 Loss: 1.926 | Acc: 28.755% (2282/7936)\n","62 391 Loss: 1.921 | Acc: 29.005% (2339/8064)\n","63 391 Loss: 1.917 | Acc: 29.089% (2383/8192)\n","64 391 Loss: 1.913 | Acc: 29.135% (2424/8320)\n","65 391 Loss: 1.911 | Acc: 29.202% (2467/8448)\n","66 391 Loss: 1.907 | Acc: 29.361% (2518/8576)\n","67 391 Loss: 1.903 | Acc: 29.538% (2571/8704)\n","68 391 Loss: 1.901 | Acc: 29.642% (2618/8832)\n","69 391 Loss: 1.897 | Acc: 29.766% (2667/8960)\n","70 391 Loss: 1.894 | Acc: 29.820% (2710/9088)\n","71 391 Loss: 1.891 | Acc: 29.915% (2757/9216)\n","72 391 Loss: 1.888 | Acc: 29.966% (2800/9344)\n","73 391 Loss: 1.884 | Acc: 30.078% (2849/9472)\n","74 391 Loss: 1.883 | Acc: 30.052% (2885/9600)\n","75 391 Loss: 1.881 | Acc: 30.078% (2926/9728)\n","76 391 Loss: 1.877 | Acc: 30.215% (2978/9856)\n","77 391 Loss: 1.875 | Acc: 30.298% (3025/9984)\n","78 391 Loss: 1.870 | Acc: 30.548% (3089/10112)\n","79 391 Loss: 1.866 | Acc: 30.713% (3145/10240)\n","80 391 Loss: 1.861 | Acc: 30.980% (3212/10368)\n","81 391 Loss: 1.857 | Acc: 31.117% (3266/10496)\n","82 391 Loss: 1.852 | Acc: 31.278% (3323/10624)\n","83 391 Loss: 1.849 | Acc: 31.390% (3375/10752)\n","84 391 Loss: 1.848 | Acc: 31.480% (3425/10880)\n","85 391 Loss: 1.847 | Acc: 31.577% (3476/11008)\n","86 391 Loss: 1.846 | Acc: 31.627% (3522/11136)\n","87 391 Loss: 1.845 | Acc: 31.676% (3568/11264)\n","88 391 Loss: 1.841 | Acc: 31.803% (3623/11392)\n","89 391 Loss: 1.837 | Acc: 31.944% (3680/11520)\n","90 391 Loss: 1.834 | Acc: 31.997% (3727/11648)\n","91 391 Loss: 1.833 | Acc: 32.057% (3775/11776)\n","92 391 Loss: 1.832 | Acc: 32.115% (3823/11904)\n","93 391 Loss: 1.829 | Acc: 32.239% (3879/12032)\n","94 391 Loss: 1.825 | Acc: 32.368% (3936/12160)\n","95 391 Loss: 1.820 | Acc: 32.544% (3999/12288)\n","96 391 Loss: 1.818 | Acc: 32.611% (4049/12416)\n","97 391 Loss: 1.816 | Acc: 32.645% (4095/12544)\n","98 391 Loss: 1.813 | Acc: 32.741% (4149/12672)\n","99 391 Loss: 1.810 | Acc: 32.836% (4203/12800)\n","100 391 Loss: 1.810 | Acc: 32.867% (4249/12928)\n","101 391 Loss: 1.807 | Acc: 32.966% (4304/13056)\n","102 391 Loss: 1.805 | Acc: 33.070% (4360/13184)\n","103 391 Loss: 1.801 | Acc: 33.203% (4420/13312)\n","104 391 Loss: 1.797 | Acc: 33.326% (4479/13440)\n","105 391 Loss: 1.795 | Acc: 33.380% (4529/13568)\n","106 391 Loss: 1.792 | Acc: 33.426% (4578/13696)\n","107 391 Loss: 1.790 | Acc: 33.529% (4635/13824)\n","108 391 Loss: 1.790 | Acc: 33.529% (4678/13952)\n","109 391 Loss: 1.789 | Acc: 33.594% (4730/14080)\n","110 391 Loss: 1.786 | Acc: 33.699% (4788/14208)\n","111 391 Loss: 1.784 | Acc: 33.754% (4839/14336)\n","112 391 Loss: 1.782 | Acc: 33.815% (4891/14464)\n","113 391 Loss: 1.780 | Acc: 33.882% (4944/14592)\n","114 391 Loss: 1.776 | Acc: 33.995% (5004/14720)\n","115 391 Loss: 1.774 | Acc: 34.079% (5060/14848)\n","116 391 Loss: 1.772 | Acc: 34.155% (5115/14976)\n","117 391 Loss: 1.771 | Acc: 34.209% (5167/15104)\n","118 391 Loss: 1.770 | Acc: 34.257% (5218/15232)\n","119 391 Loss: 1.768 | Acc: 34.303% (5269/15360)\n","120 391 Loss: 1.766 | Acc: 34.381% (5325/15488)\n","121 391 Loss: 1.764 | Acc: 34.413% (5374/15616)\n","122 391 Loss: 1.763 | Acc: 34.489% (5430/15744)\n","123 391 Loss: 1.761 | Acc: 34.539% (5482/15872)\n","124 391 Loss: 1.759 | Acc: 34.625% (5540/16000)\n","125 391 Loss: 1.757 | Acc: 34.685% (5594/16128)\n","126 391 Loss: 1.756 | Acc: 34.787% (5655/16256)\n","127 391 Loss: 1.754 | Acc: 34.845% (5709/16384)\n","128 391 Loss: 1.754 | Acc: 34.866% (5757/16512)\n","129 391 Loss: 1.753 | Acc: 34.886% (5805/16640)\n","130 391 Loss: 1.751 | Acc: 35.013% (5871/16768)\n","131 391 Loss: 1.750 | Acc: 35.050% (5922/16896)\n","132 391 Loss: 1.749 | Acc: 35.121% (5979/17024)\n","133 391 Loss: 1.747 | Acc: 35.197% (6037/17152)\n","134 391 Loss: 1.745 | Acc: 35.312% (6102/17280)\n","135 391 Loss: 1.743 | Acc: 35.352% (6154/17408)\n","136 391 Loss: 1.741 | Acc: 35.424% (6212/17536)\n","137 391 Loss: 1.739 | Acc: 35.496% (6270/17664)\n","138 391 Loss: 1.737 | Acc: 35.578% (6330/17792)\n","139 391 Loss: 1.735 | Acc: 35.631% (6385/17920)\n","140 391 Loss: 1.734 | Acc: 35.694% (6442/18048)\n","141 391 Loss: 1.732 | Acc: 35.761% (6500/18176)\n","142 391 Loss: 1.731 | Acc: 35.817% (6556/18304)\n","143 391 Loss: 1.729 | Acc: 35.905% (6618/18432)\n","144 391 Loss: 1.729 | Acc: 35.959% (6674/18560)\n","145 391 Loss: 1.727 | Acc: 36.007% (6729/18688)\n","146 391 Loss: 1.726 | Acc: 36.081% (6789/18816)\n","147 391 Loss: 1.726 | Acc: 36.085% (6836/18944)\n","148 391 Loss: 1.724 | Acc: 36.158% (6896/19072)\n","149 391 Loss: 1.723 | Acc: 36.234% (6957/19200)\n","150 391 Loss: 1.723 | Acc: 36.269% (7010/19328)\n","151 391 Loss: 1.722 | Acc: 36.313% (7065/19456)\n","152 391 Loss: 1.721 | Acc: 36.351% (7119/19584)\n","153 391 Loss: 1.719 | Acc: 36.435% (7182/19712)\n","154 391 Loss: 1.717 | Acc: 36.492% (7240/19840)\n","155 391 Loss: 1.715 | Acc: 36.569% (7302/19968)\n","156 391 Loss: 1.713 | Acc: 36.659% (7367/20096)\n","157 391 Loss: 1.711 | Acc: 36.768% (7436/20224)\n","158 391 Loss: 1.709 | Acc: 36.822% (7494/20352)\n","159 391 Loss: 1.708 | Acc: 36.904% (7558/20480)\n","160 391 Loss: 1.707 | Acc: 36.947% (7614/20608)\n","161 391 Loss: 1.705 | Acc: 36.998% (7672/20736)\n","162 391 Loss: 1.703 | Acc: 37.078% (7736/20864)\n","163 391 Loss: 1.702 | Acc: 37.119% (7792/20992)\n","164 391 Loss: 1.701 | Acc: 37.150% (7846/21120)\n","165 391 Loss: 1.700 | Acc: 37.208% (7906/21248)\n","166 391 Loss: 1.698 | Acc: 37.275% (7968/21376)\n","167 391 Loss: 1.698 | Acc: 37.300% (8021/21504)\n","168 391 Loss: 1.696 | Acc: 37.338% (8077/21632)\n","169 391 Loss: 1.694 | Acc: 37.367% (8131/21760)\n","170 391 Loss: 1.692 | Acc: 37.450% (8197/21888)\n","171 391 Loss: 1.691 | Acc: 37.500% (8256/22016)\n","172 391 Loss: 1.689 | Acc: 37.577% (8321/22144)\n","173 391 Loss: 1.688 | Acc: 37.639% (8383/22272)\n","174 391 Loss: 1.688 | Acc: 37.665% (8437/22400)\n","175 391 Loss: 1.687 | Acc: 37.669% (8486/22528)\n","176 391 Loss: 1.685 | Acc: 37.747% (8552/22656)\n","177 391 Loss: 1.683 | Acc: 37.803% (8613/22784)\n","178 391 Loss: 1.682 | Acc: 37.880% (8679/22912)\n","179 391 Loss: 1.680 | Acc: 37.943% (8742/23040)\n","180 391 Loss: 1.678 | Acc: 38.014% (8807/23168)\n","181 391 Loss: 1.676 | Acc: 38.067% (8868/23296)\n","182 391 Loss: 1.673 | Acc: 38.196% (8947/23424)\n","183 391 Loss: 1.672 | Acc: 38.256% (9010/23552)\n","184 391 Loss: 1.671 | Acc: 38.264% (9061/23680)\n","185 391 Loss: 1.669 | Acc: 38.315% (9122/23808)\n","186 391 Loss: 1.668 | Acc: 38.390% (9189/23936)\n","187 391 Loss: 1.667 | Acc: 38.427% (9247/24064)\n","188 391 Loss: 1.665 | Acc: 38.480% (9309/24192)\n","189 391 Loss: 1.664 | Acc: 38.520% (9368/24320)\n","190 391 Loss: 1.662 | Acc: 38.588% (9434/24448)\n","191 391 Loss: 1.660 | Acc: 38.643% (9497/24576)\n","192 391 Loss: 1.659 | Acc: 38.678% (9555/24704)\n","193 391 Loss: 1.658 | Acc: 38.732% (9618/24832)\n","194 391 Loss: 1.656 | Acc: 38.810% (9687/24960)\n","195 391 Loss: 1.654 | Acc: 38.899% (9759/25088)\n","196 391 Loss: 1.653 | Acc: 38.971% (9827/25216)\n","197 391 Loss: 1.651 | Acc: 39.003% (9885/25344)\n","198 391 Loss: 1.650 | Acc: 39.090% (9957/25472)\n","199 391 Loss: 1.648 | Acc: 39.156% (10024/25600)\n","200 391 Loss: 1.647 | Acc: 39.210% (10088/25728)\n","201 391 Loss: 1.646 | Acc: 39.244% (10147/25856)\n","202 391 Loss: 1.644 | Acc: 39.278% (10206/25984)\n","203 391 Loss: 1.643 | Acc: 39.334% (10271/26112)\n","204 391 Loss: 1.641 | Acc: 39.341% (10323/26240)\n","205 391 Loss: 1.641 | Acc: 39.358% (10378/26368)\n","206 391 Loss: 1.640 | Acc: 39.364% (10430/26496)\n","207 391 Loss: 1.640 | Acc: 39.393% (10488/26624)\n","208 391 Loss: 1.638 | Acc: 39.421% (10546/26752)\n","209 391 Loss: 1.637 | Acc: 39.487% (10614/26880)\n","210 391 Loss: 1.635 | Acc: 39.544% (10680/27008)\n","211 391 Loss: 1.633 | Acc: 39.630% (10754/27136)\n","212 391 Loss: 1.632 | Acc: 39.690% (10821/27264)\n","213 391 Loss: 1.631 | Acc: 39.745% (10887/27392)\n","214 391 Loss: 1.629 | Acc: 39.800% (10953/27520)\n","215 391 Loss: 1.628 | Acc: 39.858% (11020/27648)\n","216 391 Loss: 1.626 | Acc: 39.909% (11085/27776)\n","217 391 Loss: 1.625 | Acc: 39.944% (11146/27904)\n","218 391 Loss: 1.623 | Acc: 40.008% (11215/28032)\n","219 391 Loss: 1.622 | Acc: 40.089% (11289/28160)\n","220 391 Loss: 1.621 | Acc: 40.119% (11349/28288)\n","221 391 Loss: 1.620 | Acc: 40.164% (11413/28416)\n","222 391 Loss: 1.619 | Acc: 40.236% (11485/28544)\n","223 391 Loss: 1.617 | Acc: 40.283% (11550/28672)\n","224 391 Loss: 1.615 | Acc: 40.354% (11622/28800)\n","225 391 Loss: 1.613 | Acc: 40.418% (11692/28928)\n","226 391 Loss: 1.612 | Acc: 40.453% (11754/29056)\n","227 391 Loss: 1.610 | Acc: 40.498% (11819/29184)\n","228 391 Loss: 1.608 | Acc: 40.584% (11896/29312)\n","229 391 Loss: 1.607 | Acc: 40.652% (11968/29440)\n","230 391 Loss: 1.606 | Acc: 40.699% (12034/29568)\n","231 391 Loss: 1.604 | Acc: 40.763% (12105/29696)\n","232 391 Loss: 1.602 | Acc: 40.819% (12174/29824)\n","233 391 Loss: 1.601 | Acc: 40.875% (12243/29952)\n","234 391 Loss: 1.601 | Acc: 40.898% (12302/30080)\n","235 391 Loss: 1.599 | Acc: 40.943% (12368/30208)\n","236 391 Loss: 1.598 | Acc: 40.997% (12437/30336)\n","237 391 Loss: 1.597 | Acc: 41.042% (12503/30464)\n","238 391 Loss: 1.596 | Acc: 41.092% (12571/30592)\n","239 391 Loss: 1.594 | Acc: 41.133% (12636/30720)\n","240 391 Loss: 1.593 | Acc: 41.186% (12705/30848)\n","241 391 Loss: 1.592 | Acc: 41.222% (12769/30976)\n","242 391 Loss: 1.591 | Acc: 41.252% (12831/31104)\n","243 391 Loss: 1.589 | Acc: 41.323% (12906/31232)\n","244 391 Loss: 1.588 | Acc: 41.390% (12980/31360)\n","245 391 Loss: 1.587 | Acc: 41.425% (13044/31488)\n","246 391 Loss: 1.586 | Acc: 41.457% (13107/31616)\n","247 391 Loss: 1.586 | Acc: 41.488% (13170/31744)\n","248 391 Loss: 1.584 | Acc: 41.538% (13239/31872)\n","249 391 Loss: 1.583 | Acc: 41.584% (13307/32000)\n","250 391 Loss: 1.582 | Acc: 41.612% (13369/32128)\n","251 391 Loss: 1.582 | Acc: 41.620% (13425/32256)\n","252 391 Loss: 1.581 | Acc: 41.663% (13492/32384)\n","253 391 Loss: 1.580 | Acc: 41.729% (13567/32512)\n","254 391 Loss: 1.578 | Acc: 41.801% (13644/32640)\n","255 391 Loss: 1.577 | Acc: 41.855% (13715/32768)\n","256 391 Loss: 1.576 | Acc: 41.899% (13783/32896)\n","257 391 Loss: 1.575 | Acc: 41.948% (13853/33024)\n","258 391 Loss: 1.573 | Acc: 42.007% (13926/33152)\n","259 391 Loss: 1.572 | Acc: 42.028% (13987/33280)\n","260 391 Loss: 1.571 | Acc: 42.101% (14065/33408)\n","261 391 Loss: 1.569 | Acc: 42.155% (14137/33536)\n","262 391 Loss: 1.568 | Acc: 42.199% (14206/33664)\n","263 391 Loss: 1.567 | Acc: 42.253% (14278/33792)\n","264 391 Loss: 1.566 | Acc: 42.308% (14351/33920)\n","265 391 Loss: 1.566 | Acc: 42.349% (14419/34048)\n","266 391 Loss: 1.565 | Acc: 42.381% (14484/34176)\n","267 391 Loss: 1.564 | Acc: 42.389% (14541/34304)\n","268 391 Loss: 1.563 | Acc: 42.440% (14613/34432)\n","269 391 Loss: 1.563 | Acc: 42.477% (14680/34560)\n","270 391 Loss: 1.562 | Acc: 42.507% (14745/34688)\n","271 391 Loss: 1.561 | Acc: 42.544% (14812/34816)\n","272 391 Loss: 1.560 | Acc: 42.585% (14881/34944)\n","273 391 Loss: 1.558 | Acc: 42.641% (14955/35072)\n","274 391 Loss: 1.558 | Acc: 42.670% (15020/35200)\n","275 391 Loss: 1.557 | Acc: 42.725% (15094/35328)\n","276 391 Loss: 1.556 | Acc: 42.735% (15152/35456)\n","277 391 Loss: 1.556 | Acc: 42.755% (15214/35584)\n","278 391 Loss: 1.555 | Acc: 42.792% (15282/35712)\n","279 391 Loss: 1.554 | Acc: 42.810% (15343/35840)\n","280 391 Loss: 1.553 | Acc: 42.852% (15413/35968)\n","281 391 Loss: 1.552 | Acc: 42.886% (15480/36096)\n","282 391 Loss: 1.550 | Acc: 42.925% (15549/36224)\n","283 391 Loss: 1.549 | Acc: 42.958% (15616/36352)\n","284 391 Loss: 1.549 | Acc: 42.985% (15681/36480)\n","285 391 Loss: 1.548 | Acc: 43.021% (15749/36608)\n","286 391 Loss: 1.547 | Acc: 43.048% (15814/36736)\n","287 391 Loss: 1.546 | Acc: 43.094% (15886/36864)\n","288 391 Loss: 1.545 | Acc: 43.126% (15953/36992)\n","289 391 Loss: 1.544 | Acc: 43.157% (16020/37120)\n","290 391 Loss: 1.543 | Acc: 43.200% (16091/37248)\n","291 391 Loss: 1.542 | Acc: 43.252% (16166/37376)\n","292 391 Loss: 1.541 | Acc: 43.315% (16245/37504)\n","293 391 Loss: 1.540 | Acc: 43.365% (16319/37632)\n","294 391 Loss: 1.539 | Acc: 43.408% (16391/37760)\n","295 391 Loss: 1.537 | Acc: 43.481% (16474/37888)\n","296 391 Loss: 1.536 | Acc: 43.532% (16549/38016)\n","297 391 Loss: 1.535 | Acc: 43.566% (16618/38144)\n","298 391 Loss: 1.534 | Acc: 43.585% (16681/38272)\n","299 391 Loss: 1.533 | Acc: 43.635% (16756/38400)\n","300 391 Loss: 1.532 | Acc: 43.659% (16821/38528)\n","301 391 Loss: 1.532 | Acc: 43.693% (16890/38656)\n","302 391 Loss: 1.531 | Acc: 43.729% (16960/38784)\n","303 391 Loss: 1.530 | Acc: 43.778% (17035/38912)\n","304 391 Loss: 1.528 | Acc: 43.847% (17118/39040)\n","305 391 Loss: 1.527 | Acc: 43.867% (17182/39168)\n","306 391 Loss: 1.526 | Acc: 43.908% (17254/39296)\n","307 391 Loss: 1.526 | Acc: 43.935% (17321/39424)\n","308 391 Loss: 1.525 | Acc: 43.985% (17397/39552)\n","309 391 Loss: 1.523 | Acc: 44.040% (17475/39680)\n","310 391 Loss: 1.523 | Acc: 44.056% (17538/39808)\n","311 391 Loss: 1.522 | Acc: 44.091% (17608/39936)\n","312 391 Loss: 1.521 | Acc: 44.119% (17676/40064)\n","313 391 Loss: 1.520 | Acc: 44.161% (17749/40192)\n","314 391 Loss: 1.519 | Acc: 44.201% (17822/40320)\n","315 391 Loss: 1.518 | Acc: 44.242% (17895/40448)\n","316 391 Loss: 1.518 | Acc: 44.258% (17958/40576)\n","317 391 Loss: 1.517 | Acc: 44.293% (18029/40704)\n","318 391 Loss: 1.516 | Acc: 44.330% (18101/40832)\n","319 391 Loss: 1.514 | Acc: 44.382% (18179/40960)\n","320 391 Loss: 1.514 | Acc: 44.424% (18253/41088)\n","321 391 Loss: 1.512 | Acc: 44.485% (18335/41216)\n","322 391 Loss: 1.511 | Acc: 44.507% (18401/41344)\n","323 391 Loss: 1.510 | Acc: 44.551% (18476/41472)\n","324 391 Loss: 1.509 | Acc: 44.589% (18549/41600)\n","325 391 Loss: 1.509 | Acc: 44.620% (18619/41728)\n","326 391 Loss: 1.508 | Acc: 44.644% (18686/41856)\n","327 391 Loss: 1.507 | Acc: 44.700% (18767/41984)\n","328 391 Loss: 1.505 | Acc: 44.750% (18845/42112)\n","329 391 Loss: 1.504 | Acc: 44.804% (18925/42240)\n","330 391 Loss: 1.503 | Acc: 44.852% (19003/42368)\n","331 391 Loss: 1.503 | Acc: 44.875% (19070/42496)\n","332 391 Loss: 1.501 | Acc: 44.921% (19147/42624)\n","333 391 Loss: 1.500 | Acc: 44.955% (19219/42752)\n","334 391 Loss: 1.499 | Acc: 45.005% (19298/42880)\n","335 391 Loss: 1.498 | Acc: 45.040% (19371/43008)\n","336 391 Loss: 1.496 | Acc: 45.122% (19464/43136)\n","337 391 Loss: 1.495 | Acc: 45.174% (19544/43264)\n","338 391 Loss: 1.494 | Acc: 45.209% (19617/43392)\n","339 391 Loss: 1.493 | Acc: 45.230% (19684/43520)\n","340 391 Loss: 1.492 | Acc: 45.255% (19753/43648)\n","341 391 Loss: 1.491 | Acc: 45.319% (19839/43776)\n","342 391 Loss: 1.490 | Acc: 45.369% (19919/43904)\n","343 391 Loss: 1.489 | Acc: 45.431% (20004/44032)\n","344 391 Loss: 1.487 | Acc: 45.471% (20080/44160)\n","345 391 Loss: 1.487 | Acc: 45.498% (20150/44288)\n","346 391 Loss: 1.486 | Acc: 45.540% (20227/44416)\n","347 391 Loss: 1.484 | Acc: 45.607% (20315/44544)\n","348 391 Loss: 1.483 | Acc: 45.635% (20386/44672)\n","349 391 Loss: 1.482 | Acc: 45.661% (20456/44800)\n","350 391 Loss: 1.482 | Acc: 45.682% (20524/44928)\n","351 391 Loss: 1.481 | Acc: 45.721% (20600/45056)\n","352 391 Loss: 1.480 | Acc: 45.757% (20675/45184)\n","353 391 Loss: 1.479 | Acc: 45.772% (20740/45312)\n","354 391 Loss: 1.478 | Acc: 45.819% (20820/45440)\n","355 391 Loss: 1.477 | Acc: 45.870% (20902/45568)\n","356 391 Loss: 1.476 | Acc: 45.919% (20983/45696)\n","357 391 Loss: 1.475 | Acc: 45.956% (21059/45824)\n","358 391 Loss: 1.474 | Acc: 45.991% (21134/45952)\n","359 391 Loss: 1.473 | Acc: 46.037% (21214/46080)\n","360 391 Loss: 1.473 | Acc: 46.076% (21291/46208)\n","361 391 Loss: 1.472 | Acc: 46.100% (21361/46336)\n","362 391 Loss: 1.470 | Acc: 46.132% (21435/46464)\n","363 391 Loss: 1.470 | Acc: 46.150% (21502/46592)\n","364 391 Loss: 1.469 | Acc: 46.196% (21583/46720)\n","365 391 Loss: 1.468 | Acc: 46.196% (21642/46848)\n","366 391 Loss: 1.467 | Acc: 46.226% (21715/46976)\n","367 391 Loss: 1.466 | Acc: 46.259% (21790/47104)\n","368 391 Loss: 1.466 | Acc: 46.293% (21865/47232)\n","369 391 Loss: 1.464 | Acc: 46.353% (21953/47360)\n","370 391 Loss: 1.464 | Acc: 46.370% (22020/47488)\n","371 391 Loss: 1.464 | Acc: 46.392% (22090/47616)\n","372 391 Loss: 1.463 | Acc: 46.423% (22164/47744)\n","373 391 Loss: 1.463 | Acc: 46.428% (22226/47872)\n","374 391 Loss: 1.462 | Acc: 46.456% (22299/48000)\n","375 391 Loss: 1.462 | Acc: 46.486% (22373/48128)\n","376 391 Loss: 1.461 | Acc: 46.514% (22446/48256)\n","377 391 Loss: 1.460 | Acc: 46.536% (22516/48384)\n","378 391 Loss: 1.459 | Acc: 46.564% (22589/48512)\n","379 391 Loss: 1.458 | Acc: 46.608% (22670/48640)\n","380 391 Loss: 1.458 | Acc: 46.637% (22744/48768)\n","381 391 Loss: 1.457 | Acc: 46.664% (22817/48896)\n","382 391 Loss: 1.456 | Acc: 46.708% (22898/49024)\n","383 391 Loss: 1.455 | Acc: 46.759% (22983/49152)\n","384 391 Loss: 1.454 | Acc: 46.780% (23053/49280)\n","385 391 Loss: 1.454 | Acc: 46.810% (23128/49408)\n","386 391 Loss: 1.453 | Acc: 46.845% (23205/49536)\n","387 391 Loss: 1.452 | Acc: 46.895% (23290/49664)\n","388 391 Loss: 1.451 | Acc: 46.939% (23372/49792)\n","389 391 Loss: 1.450 | Acc: 46.963% (23444/49920)\n","390 391 Loss: 1.450 | Acc: 46.970% (23485/50000)\n","0 100 Loss: 1.238 | Acc: 60.000% (60/100)\n","1 100 Loss: 1.294 | Acc: 55.500% (111/200)\n","2 100 Loss: 1.273 | Acc: 55.333% (166/300)\n","3 100 Loss: 1.243 | Acc: 57.750% (231/400)\n","4 100 Loss: 1.221 | Acc: 57.200% (286/500)\n","5 100 Loss: 1.201 | Acc: 58.000% (348/600)\n","6 100 Loss: 1.212 | Acc: 57.714% (404/700)\n","7 100 Loss: 1.243 | Acc: 56.375% (451/800)\n","8 100 Loss: 1.224 | Acc: 57.556% (518/900)\n","9 100 Loss: 1.218 | Acc: 57.800% (578/1000)\n","10 100 Loss: 1.208 | Acc: 58.000% (638/1100)\n","11 100 Loss: 1.195 | Acc: 58.417% (701/1200)\n","12 100 Loss: 1.202 | Acc: 58.308% (758/1300)\n","13 100 Loss: 1.191 | Acc: 58.357% (817/1400)\n","14 100 Loss: 1.195 | Acc: 58.333% (875/1500)\n","15 100 Loss: 1.207 | Acc: 58.000% (928/1600)\n","16 100 Loss: 1.208 | Acc: 57.706% (981/1700)\n","17 100 Loss: 1.202 | Acc: 57.833% (1041/1800)\n","18 100 Loss: 1.200 | Acc: 58.000% (1102/1900)\n","19 100 Loss: 1.210 | Acc: 57.800% (1156/2000)\n","20 100 Loss: 1.210 | Acc: 57.524% (1208/2100)\n","21 100 Loss: 1.205 | Acc: 57.682% (1269/2200)\n","22 100 Loss: 1.201 | Acc: 57.652% (1326/2300)\n","23 100 Loss: 1.205 | Acc: 57.667% (1384/2400)\n","24 100 Loss: 1.205 | Acc: 57.680% (1442/2500)\n","25 100 Loss: 1.213 | Acc: 57.346% (1491/2600)\n","26 100 Loss: 1.209 | Acc: 57.333% (1548/2700)\n","27 100 Loss: 1.215 | Acc: 57.357% (1606/2800)\n","28 100 Loss: 1.214 | Acc: 57.483% (1667/2900)\n","29 100 Loss: 1.205 | Acc: 57.733% (1732/3000)\n","30 100 Loss: 1.205 | Acc: 57.742% (1790/3100)\n","31 100 Loss: 1.201 | Acc: 57.875% (1852/3200)\n","32 100 Loss: 1.202 | Acc: 57.939% (1912/3300)\n","33 100 Loss: 1.205 | Acc: 57.824% (1966/3400)\n","34 100 Loss: 1.210 | Acc: 57.657% (2018/3500)\n","35 100 Loss: 1.206 | Acc: 57.833% (2082/3600)\n","36 100 Loss: 1.214 | Acc: 57.595% (2131/3700)\n","37 100 Loss: 1.215 | Acc: 57.605% (2189/3800)\n","38 100 Loss: 1.216 | Acc: 57.564% (2245/3900)\n","39 100 Loss: 1.216 | Acc: 57.575% (2303/4000)\n","40 100 Loss: 1.215 | Acc: 57.634% (2363/4100)\n","41 100 Loss: 1.218 | Acc: 57.619% (2420/4200)\n","42 100 Loss: 1.216 | Acc: 57.605% (2477/4300)\n","43 100 Loss: 1.221 | Acc: 57.432% (2527/4400)\n","44 100 Loss: 1.219 | Acc: 57.511% (2588/4500)\n","45 100 Loss: 1.219 | Acc: 57.500% (2645/4600)\n","46 100 Loss: 1.216 | Acc: 57.596% (2707/4700)\n","47 100 Loss: 1.214 | Acc: 57.646% (2767/4800)\n","48 100 Loss: 1.213 | Acc: 57.673% (2826/4900)\n","49 100 Loss: 1.209 | Acc: 57.860% (2893/5000)\n","50 100 Loss: 1.209 | Acc: 57.863% (2951/5100)\n","51 100 Loss: 1.208 | Acc: 57.808% (3006/5200)\n","52 100 Loss: 1.205 | Acc: 57.792% (3063/5300)\n","53 100 Loss: 1.207 | Acc: 57.796% (3121/5400)\n","54 100 Loss: 1.208 | Acc: 57.709% (3174/5500)\n","55 100 Loss: 1.211 | Acc: 57.625% (3227/5600)\n","56 100 Loss: 1.209 | Acc: 57.772% (3293/5700)\n","57 100 Loss: 1.205 | Acc: 57.759% (3350/5800)\n","58 100 Loss: 1.210 | Acc: 57.559% (3396/5900)\n","59 100 Loss: 1.211 | Acc: 57.533% (3452/6000)\n","60 100 Loss: 1.208 | Acc: 57.639% (3516/6100)\n","61 100 Loss: 1.209 | Acc: 57.597% (3571/6200)\n","62 100 Loss: 1.210 | Acc: 57.540% (3625/6300)\n","63 100 Loss: 1.209 | Acc: 57.531% (3682/6400)\n","64 100 Loss: 1.209 | Acc: 57.477% (3736/6500)\n","65 100 Loss: 1.209 | Acc: 57.485% (3794/6600)\n","66 100 Loss: 1.210 | Acc: 57.373% (3844/6700)\n","67 100 Loss: 1.211 | Acc: 57.294% (3896/6800)\n","68 100 Loss: 1.212 | Acc: 57.232% (3949/6900)\n","69 100 Loss: 1.214 | Acc: 57.143% (4000/7000)\n","70 100 Loss: 1.213 | Acc: 57.197% (4061/7100)\n","71 100 Loss: 1.210 | Acc: 57.361% (4130/7200)\n","72 100 Loss: 1.207 | Acc: 57.411% (4191/7300)\n","73 100 Loss: 1.203 | Acc: 57.486% (4254/7400)\n","74 100 Loss: 1.204 | Acc: 57.520% (4314/7500)\n","75 100 Loss: 1.203 | Acc: 57.605% (4378/7600)\n","76 100 Loss: 1.201 | Acc: 57.727% (4445/7700)\n","77 100 Loss: 1.202 | Acc: 57.731% (4503/7800)\n","78 100 Loss: 1.200 | Acc: 57.835% (4569/7900)\n","79 100 Loss: 1.202 | Acc: 57.825% (4626/8000)\n","80 100 Loss: 1.201 | Acc: 57.889% (4689/8100)\n","81 100 Loss: 1.203 | Acc: 57.780% (4738/8200)\n","82 100 Loss: 1.203 | Acc: 57.819% (4799/8300)\n","83 100 Loss: 1.205 | Acc: 57.726% (4849/8400)\n","84 100 Loss: 1.206 | Acc: 57.706% (4905/8500)\n","85 100 Loss: 1.206 | Acc: 57.709% (4963/8600)\n","86 100 Loss: 1.207 | Acc: 57.655% (5016/8700)\n","87 100 Loss: 1.207 | Acc: 57.682% (5076/8800)\n","88 100 Loss: 1.209 | Acc: 57.584% (5125/8900)\n","89 100 Loss: 1.211 | Acc: 57.567% (5181/9000)\n","90 100 Loss: 1.210 | Acc: 57.571% (5239/9100)\n","91 100 Loss: 1.206 | Acc: 57.674% (5306/9200)\n","92 100 Loss: 1.206 | Acc: 57.677% (5364/9300)\n","93 100 Loss: 1.207 | Acc: 57.564% (5411/9400)\n","94 100 Loss: 1.206 | Acc: 57.632% (5475/9500)\n","95 100 Loss: 1.207 | Acc: 57.604% (5530/9600)\n","96 100 Loss: 1.206 | Acc: 57.608% (5588/9700)\n","97 100 Loss: 1.207 | Acc: 57.602% (5645/9800)\n","98 100 Loss: 1.208 | Acc: 57.545% (5697/9900)\n","99 100 Loss: 1.208 | Acc: 57.470% (5747/10000)\n","Saving..\n","\n","Epoch: 1\n","0 391 Loss: 1.216 | Acc: 53.906% (69/128)\n","1 391 Loss: 1.106 | Acc: 59.375% (152/256)\n","2 391 Loss: 1.103 | Acc: 60.938% (234/384)\n","3 391 Loss: 1.066 | Acc: 62.500% (320/512)\n","4 391 Loss: 1.058 | Acc: 63.594% (407/640)\n","5 391 Loss: 1.053 | Acc: 63.932% (491/768)\n","6 391 Loss: 1.058 | Acc: 63.504% (569/896)\n","7 391 Loss: 1.072 | Acc: 62.402% (639/1024)\n","8 391 Loss: 1.076 | Acc: 61.892% (713/1152)\n","9 391 Loss: 1.069 | Acc: 62.188% (796/1280)\n","10 391 Loss: 1.085 | Acc: 61.648% (868/1408)\n","11 391 Loss: 1.086 | Acc: 61.198% (940/1536)\n","12 391 Loss: 1.099 | Acc: 61.058% (1016/1664)\n","13 391 Loss: 1.107 | Acc: 60.547% (1085/1792)\n","14 391 Loss: 1.110 | Acc: 60.312% (1158/1920)\n","15 391 Loss: 1.101 | Acc: 60.449% (1238/2048)\n","16 391 Loss: 1.099 | Acc: 60.662% (1320/2176)\n","17 391 Loss: 1.110 | Acc: 60.286% (1389/2304)\n","18 391 Loss: 1.121 | Acc: 59.827% (1455/2432)\n","19 391 Loss: 1.123 | Acc: 59.688% (1528/2560)\n","20 391 Loss: 1.117 | Acc: 60.007% (1613/2688)\n","21 391 Loss: 1.118 | Acc: 59.872% (1686/2816)\n","22 391 Loss: 1.114 | Acc: 60.054% (1768/2944)\n","23 391 Loss: 1.111 | Acc: 60.286% (1852/3072)\n","24 391 Loss: 1.109 | Acc: 60.219% (1927/3200)\n","25 391 Loss: 1.105 | Acc: 60.607% (2017/3328)\n","26 391 Loss: 1.101 | Acc: 60.735% (2099/3456)\n","27 391 Loss: 1.101 | Acc: 60.770% (2178/3584)\n","28 391 Loss: 1.097 | Acc: 60.884% (2260/3712)\n","29 391 Loss: 1.102 | Acc: 60.729% (2332/3840)\n","30 391 Loss: 1.099 | Acc: 60.811% (2413/3968)\n","31 391 Loss: 1.093 | Acc: 61.011% (2499/4096)\n","32 391 Loss: 1.091 | Acc: 61.127% (2582/4224)\n","33 391 Loss: 1.088 | Acc: 61.259% (2666/4352)\n","34 391 Loss: 1.083 | Acc: 61.451% (2753/4480)\n","35 391 Loss: 1.084 | Acc: 61.415% (2830/4608)\n","36 391 Loss: 1.084 | Acc: 61.508% (2913/4736)\n","37 391 Loss: 1.081 | Acc: 61.513% (2992/4864)\n","38 391 Loss: 1.083 | Acc: 61.358% (3063/4992)\n","39 391 Loss: 1.082 | Acc: 61.387% (3143/5120)\n","40 391 Loss: 1.080 | Acc: 61.490% (3227/5248)\n","41 391 Loss: 1.083 | Acc: 61.291% (3295/5376)\n","42 391 Loss: 1.083 | Acc: 61.283% (3373/5504)\n","43 391 Loss: 1.082 | Acc: 61.399% (3458/5632)\n","44 391 Loss: 1.077 | Acc: 61.562% (3546/5760)\n","45 391 Loss: 1.076 | Acc: 61.600% (3627/5888)\n","46 391 Loss: 1.077 | Acc: 61.553% (3703/6016)\n","47 391 Loss: 1.075 | Acc: 61.556% (3782/6144)\n","48 391 Loss: 1.073 | Acc: 61.607% (3864/6272)\n","49 391 Loss: 1.073 | Acc: 61.578% (3941/6400)\n","50 391 Loss: 1.071 | Acc: 61.596% (4021/6528)\n","51 391 Loss: 1.070 | Acc: 61.538% (4096/6656)\n","52 391 Loss: 1.072 | Acc: 61.512% (4173/6784)\n","53 391 Loss: 1.071 | Acc: 61.516% (4252/6912)\n","54 391 Loss: 1.075 | Acc: 61.392% (4322/7040)\n","55 391 Loss: 1.074 | Acc: 61.328% (4396/7168)\n","56 391 Loss: 1.075 | Acc: 61.294% (4472/7296)\n","57 391 Loss: 1.075 | Acc: 61.315% (4552/7424)\n","58 391 Loss: 1.074 | Acc: 61.361% (4634/7552)\n","59 391 Loss: 1.073 | Acc: 61.393% (4715/7680)\n","60 391 Loss: 1.071 | Acc: 61.488% (4801/7808)\n","61 391 Loss: 1.070 | Acc: 61.568% (4886/7936)\n","62 391 Loss: 1.070 | Acc: 61.570% (4965/8064)\n","63 391 Loss: 1.071 | Acc: 61.548% (5042/8192)\n","64 391 Loss: 1.069 | Acc: 61.635% (5128/8320)\n","65 391 Loss: 1.068 | Acc: 61.624% (5206/8448)\n","66 391 Loss: 1.067 | Acc: 61.660% (5288/8576)\n","67 391 Loss: 1.065 | Acc: 61.742% (5374/8704)\n","68 391 Loss: 1.066 | Acc: 61.719% (5451/8832)\n","69 391 Loss: 1.065 | Acc: 61.741% (5532/8960)\n","70 391 Loss: 1.064 | Acc: 61.763% (5613/9088)\n","71 391 Loss: 1.064 | Acc: 61.740% (5690/9216)\n","72 391 Loss: 1.062 | Acc: 61.836% (5778/9344)\n","73 391 Loss: 1.063 | Acc: 61.888% (5862/9472)\n","74 391 Loss: 1.063 | Acc: 61.906% (5943/9600)\n","75 391 Loss: 1.064 | Acc: 61.842% (6016/9728)\n","76 391 Loss: 1.065 | Acc: 61.780% (6089/9856)\n","77 391 Loss: 1.063 | Acc: 61.879% (6178/9984)\n","78 391 Loss: 1.062 | Acc: 61.877% (6257/10112)\n","79 391 Loss: 1.061 | Acc: 61.963% (6345/10240)\n","80 391 Loss: 1.061 | Acc: 61.989% (6427/10368)\n","81 391 Loss: 1.065 | Acc: 61.852% (6492/10496)\n","82 391 Loss: 1.064 | Acc: 61.851% (6571/10624)\n","83 391 Loss: 1.062 | Acc: 61.886% (6654/10752)\n","84 391 Loss: 1.062 | Acc: 61.875% (6732/10880)\n","85 391 Loss: 1.061 | Acc: 61.910% (6815/11008)\n","86 391 Loss: 1.060 | Acc: 62.015% (6906/11136)\n","87 391 Loss: 1.058 | Acc: 62.074% (6992/11264)\n","88 391 Loss: 1.058 | Acc: 62.070% (7071/11392)\n","89 391 Loss: 1.055 | Acc: 62.161% (7161/11520)\n","90 391 Loss: 1.054 | Acc: 62.200% (7245/11648)\n","91 391 Loss: 1.054 | Acc: 62.211% (7326/11776)\n","92 391 Loss: 1.053 | Acc: 62.248% (7410/11904)\n","93 391 Loss: 1.050 | Acc: 62.350% (7502/12032)\n","94 391 Loss: 1.051 | Acc: 62.344% (7581/12160)\n","95 391 Loss: 1.049 | Acc: 62.419% (7670/12288)\n","96 391 Loss: 1.049 | Acc: 62.436% (7752/12416)\n","97 391 Loss: 1.047 | Acc: 62.564% (7848/12544)\n","98 391 Loss: 1.046 | Acc: 62.571% (7929/12672)\n","99 391 Loss: 1.047 | Acc: 62.586% (8011/12800)\n","100 391 Loss: 1.046 | Acc: 62.608% (8094/12928)\n"],"name":"stdout"}]}]}